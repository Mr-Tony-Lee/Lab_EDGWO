# Lab_EDGWO 
## EDGWO_G
我們在實作完 EDGWO 後，首先，我們思考能不能在初期，去探索更多區域，然後在後期的時候，能更專注於收斂，所以我們修改了原文中提及的 Local 跟 Global 的開採子 (觸發的比例)，改採隨著時間去讓程式在初期時更偏好使用 Global，後期時更偏好使用 Local，再來，我們認為 alpha 狼有更好的位置，所以我們在更新位置時，讓 Alpha 狼的權重變高，期望能有更好的結果。

再來，我們對於其輸出結果進行觀察，發現 EDGWO 在很早期的時候就收斂到了一個範圍附近，但是卻遲遲沒有掉下去到最低點。

所以，我們在 EDGWO_G 中做了改良， 在 EDGWO 執行到很後面的時候，改採以 alpha 狼這個最棒的點進行梯度下降，幫助其更快收斂到最低點。
原本我們給定一個固定的 Learning Rate 去執行梯度學習，但之後，我們再經過思考之後，改採用根據情況自動調整的 Learning Rate，讓在梯度下降的過程中，如果出現優良的點，我們就給予獎勵 (Learning Rate 上升) ，反之，則給予懲罰 (Learning Rate 下降)，並不予移動。

## EDGWO_GA

我們根據 EDGWO_G 再做出一些嘗試，因為我們發現 EDGWO 在中後期時，曲線趨近平緩，似乎已經進入 Local ，所以我們的做法是，當斜率小於一定數值之後，保留 alpha 狼的位置，繼續進行梯度下降，剩下的狼重新進行隨機分布，並重新進行以上步驟，避免陷入區域最小值。

## EDGWO_GB

我們根據 EDGWO_G 再做出一些嘗試，因為我們發現 EDGWO 下降速度過於平緩，且會有 alpha 狼持續不動的情況，所以我們的做法是，當 alpha 狼持續不動時，讓她進行梯度下降，去嘗試移動到更好的位置，繼續進行 EDGWO。 再執行幾次過後，發現要讓梯度的學習率隨時間遞減，因為在初期，想讓 alpha 狼的步長增加，去達到更快的收斂，在後期，想讓alpha 狼的步常減少，達到更精確的收斂。